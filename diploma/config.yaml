model:
  name: autoencoder_upsampler
  path: diploma/models/autoencoder_upsampler.pth
  #input_size: 256
  hidden_size: 512
  #output_size: 10
  #upsample_factor: 4

train:
  device: cuda
  epochs: 1
  learning_rate: 0.001
  batch_size: 8
  optimizer:
    name: Adam
    weight_decay: 0

dataset:
  name: DIV2K
  data_path: diploma/data/DIV2K
  transform:
    - RandomHorizontalFlip
    - RandomCrop:
        size: 256
    - ToTensor



# scheduler:
#   name: StepLR
#   step_size: 10
#   gamma: 0.1

logging:
  log_dir: diploma/log/
  save_dir: /path/to/checkpoints